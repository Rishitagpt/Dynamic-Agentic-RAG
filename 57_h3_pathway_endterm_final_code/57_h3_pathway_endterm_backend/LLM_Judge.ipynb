{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N65gU-5wLUk-"
   },
   "source": [
    "# Installing the Opik Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUY_7zNcLV-c"
   },
   "outputs": [],
   "source": [
    "!pip install opik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSAtuK88C6Cq"
   },
   "source": [
    "\n",
    "#Importing Necessary Libraries and Configuring Opik\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8tRcLW60Rwi",
    "outputId": "97a56311-2ca2-4850-f5f2-546c4456b687"
   },
   "outputs": [],
   "source": [
    "import opik\n",
    "from opik import Opik, track\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import (Hallucination, AnswerRelevance)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "opik.configure(use_local=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5o2MN2CFRJN"
   },
   "source": [
    "#Setting the OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC46zsRx6fiR"
   },
   "outputs": [],
   "source": [
    "open_api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-3sa8GTGUb3"
   },
   "source": [
    "#Loading the CSV File into a DataFrame\n",
    "###In this cell, we load the CSV file into a pandas DataFrame. The CSV contains the query, LLM response, and retrieved passage, which will be used for evaluating the performance of the language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "Gmw0KIx_14hL",
    "outputId": "6e1e5207-64b0-4994-82d4-a2c52dd4873d"
   },
   "outputs": [],
   "source": [
    "file_path = \"Enter csv file path here\"\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqvKPRGuKdng"
   },
   "source": [
    "#Evaluating the LLM Responses\n",
    "In this cell, we process each row of the CSV file and evaluate the LLM response using the Hallucination and AnswerRelevance metrics. The results are saved in batches to avoid exceeding the rate limit, and progress is saved periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SlYuVzb9-6Ud",
    "outputId": "eaa918e9-47cb-4d57-a382-ea4ca135d67d"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "results = []\n",
    "metrics = [Hallucination(), AnswerRelevance()]\n",
    "batch_size = 3  # Process in batches to reduce the chance of exceeding the rate limit\n",
    "\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        row = df.iloc[i]\n",
    "        scores = {\"query\": row['Question'], \"answer\": row['Response']}\n",
    "\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                result = metric.score(\n",
    "                    input=row['Question'],\n",
    "                    output=row['Response'],\n",
    "                    context=row['passage'],\n",
    "                )\n",
    "                if isinstance(metric, Hallucination):\n",
    "                    scores[\"hallucination_score\"] = result.value\n",
    "                    scores[\"hallucination_reason\"] = result.reason\n",
    "                elif isinstance(metric, AnswerRelevance):\n",
    "                    scores[\"answer_relevance_score\"] = result.value\n",
    "                    scores[\"answer_relevance_reason\"] = result.reason\n",
    "            except Exception as e:\n",
    "                print(f\"Rate limit reached: {e}. Retrying in 15 seconds...\")\n",
    "                time.sleep(15)  # Wait and retry\n",
    "                continue\n",
    "\n",
    "        results.append(scores)\n",
    "\n",
    "        # Save progress every batch_size rows\n",
    "        if i % batch_size == 0:\n",
    "            pd.DataFrame(results).to_excel(\"intermediate_results.xlsx\", index=False)\n",
    "            print(f\"Progress saved at iteration {i}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i}: {e}. Skipping to the next row...\")\n",
    "\n",
    "# Save final results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_excel(\"final_results.xlsx\", index=False)\n",
    "print(\"Processing completed. Results saved to final_results.xlsx.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
